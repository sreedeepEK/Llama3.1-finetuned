### Fine-Tuned Llama 3.1 Model

This repository contains the code and configuration used to fine-tune the Llama 3.1-8b model.

**Deployment**

The fine-tuned model is deployed on Hugging Face, and can be [accessed directly](https://huggingface.co/sreedeepEK/lora_model) from the platform.

**Finetuning Details**

The model was fine-tuned on [alpaca-cleaned](https://huggingface.co/datasets/yahma/alpaca-cleaned) dataset.
examples. The training process used a batch size of 8, and the learning rate was set to 2e-4.
To use this fine-tuned model, simply visit the Hugging Face model hub and select the Fine-Tuned Llama 3.1 model.

## Installation

No installation required - just access the model from the Hugging Face platform!

**Acknowledgments**

This work was inspired by the original Llama 3.1 model and fine-tuning code provided by Unsloth.inc.
